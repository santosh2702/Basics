# GenAI / RAG / Prompt Engineering 

> Printâ€‘friendly oneâ€‘pager followed by pocket flashcards. Keep this as your finalâ€‘hour cram sheet.

---

## ğŸ”‘ Quick Priorities

* Default stack: **Low temp + RAG + JSON schema validation + caching**.
* Chunking: **400â€“600 tokens**, **10â€“15% overlap**, headingâ€‘aware; prefer **semantic** or **hybrid** splits.
* Retrieval: **Hybrid = BM25 (sparse) + dense embeddings**; rerank topâ€‘k.
* Evaluation: **Faithfulness (citation precision/recall)** + **Task metric (EM/F1/ROUGE)** + **Ops (P95 latency, cost)**.
* Safety: guard vs **prompt injection**, **PII leakage**, **hallucinations**; enable **refusal** when unsure.

---

## ğŸ§‘â€ğŸ’» Prompt Engineering â€” Essentials

* **Zeroâ€‘shot / Fewâ€‘shot / CoT**: tradeoff between speed vs reasoning control; add final â€œ**give concise answer**â€.
* **Schemaâ€‘first prompts**: provide JSON schema & refusal clause; validate and autoâ€‘repair if invalid.
* **Prompt chaining**: retrieve â†’ plan â†’ draft â†’ critique â†’ finalize.
* **Leakage/injection defense**: keep system serverâ€‘side, content isolation, tool allowlists, output filters.

### Reusable Prompts

**Summarization (JSON)**

```
System: You are a precise technical summarizer.
User: Summarize for a nonâ€‘technical reader in â‰¤5 bullets and 1 key quote.
Return valid JSON: {"bullets":[],"quote":""}.
If unsure, use nulls.
```

**Classification**

```
Label sentiment as one of {positive, neutral, negative}.
Return JSON {"label":"...","confidence":0â€‘1} only.
```

---

## ğŸ¤– LLM Fundamentals

* **Encoderâ€‘only (BERT)**: representations, search/classification.
* **Decoderâ€‘only (GPT/LLaMA/Mistral/Claudeâ€‘style)**: generation & tool use.
* **Sampling**: tempâ†“ = deterministic; **topâ€‘k** (fixed pool), **topâ€‘p** (prob mass).
* **Adaptation**: full fineâ€‘tune (costly) âŸ¶ **LoRA/PEFT** (efficient) âŸ¶ promptâ€‘tuning (cheapest).

---

## ğŸ“š RAG Quick Diagram

Ingest â†’ Split (400â€“600, 10â€“15% overlap) â†’ Embed â†’ Index (vector DB) â†’ Query â†’ Retrieve topâ€‘k â†’ (Rerank) â†’ Compose prompt w/ citations â†’ Generate â†’ Validate JSON â†’ Display + links.

**Vector DB options**: FAISS (local), Chroma (devâ€‘friendly), Pinecone/Weaviate/Milvus (managed/scale).

**Dense vs Sparse**: Dense = paraphrases/semantics; Sparse = exact/rare tokens. **Best: Hybrid**.

**Key knobs**: k (3â€“8), filters (metadata), reranker, cache, context window budget.

---

## ğŸ§ª Evaluation Cheatsheet

* **Retrieval**: Recall\@k, MRR, nDCG.
* **Answering**: EM/F1/ROUGE; **faithfulness** (does answer quote retrieved text?).
* **Ops**: latency (P95/P99), error rate, token cost, cache hit.

---

## â˜ï¸ Deployment Cliff Notes

* Serve via **FastAPI**; containerize; enable auth, rate limits, tracing.
* **Batch vs realtime**: throughput vs latency.
* **Monitoring**: schema validity, groundedness, jailbreak attempts, PII, cost.

---

## ğŸ›  Minimal Code Snips

**Word Frequency**

```python
from collections import Counter
Counter(t.lower() for t in text.split())
```

**Reverse String**

```python
s[::-1]
```

**JSON Parse (safe)**

```python
import json
try: obj=json.loads(s)
except json.JSONDecodeError as e: obj={"error":str(e)}
```

---

# ğŸ§  Flashcards (Front â–¸ Back)

**What is prompt engineering?** â–¸ Designing inputs/instructions to get reliable, safe, structured outputs.

**Zeroâ€‘shot vs Fewâ€‘shot vs CoT?** â–¸ No examples vs examples vs guided reasoning; CoT improves reasoning, costs tokens.

**Prompt injection?** â–¸ Inputs trying to override rules/exfiltrate secrets; mitigate with serverâ€‘side system, isolation, allowlists.

**Hallucination fixes?** â–¸ RAG + lower temp + citations + verifier pass + schema validation + allow â€œI donâ€™t knowâ€.

**Temperature/topâ€‘k/topâ€‘p?** â–¸ Temp scales randomness; topâ€‘k = best k tokens; topâ€‘p = smallest mass p set.

**Dense vs Sparse search?** â–¸ Dense = semantic; Sparse (BM25) = keyword; use **hybrid**.

**Chunking heuristics?** â–¸ 400â€“600 tokens, 10â€“15% overlap, heading/semantic aware.

**RAG steps?** â–¸ Ingest â†’ split â†’ embed â†’ index â†’ retrieve â†’ (rerank) â†’ prompt+citations â†’ generate.

**Evaluate chatbot?** â–¸ Task metric + faithfulness + UX (resolution/CSAT) + Ops (latency/cost).

**Batch vs realtime?** â–¸ Batch = throughput/cost; realtime = lowâ€‘latency SLA.

**Regularization?** â–¸ L1/L2, dropout, early stop; reduces overfitting.

**Precision/Recall/F1?** â–¸ TP/(TP+FP), TP/(TP+FN), harmonic mean.

**RF vs NN?** â–¸ RF strong on tabular/low tuning; NN flexible, dataâ€‘hungry.

**Transfer learning?** â–¸ Start from preâ€‘trained; fineâ€‘tune minimal layers (e.g., LoRA).

---

## ğŸ¯ Interview Oneâ€‘liners

* â€œWe use **hybrid retrieval** with reranking and measure **citation precision/recall** for faithfulness.â€
* â€œDefault: **low temp + schemaâ€‘first** prompts; invalid JSON triggers autoâ€‘repair cycle.â€
* â€œChunking at **\~500 tokens** with **15% overlap** balanced recall and token cost.â€
* â€œCaching at **embedding, retrieval, and generation** layers cut costs significantly.â€

---

## ğŸ§© Mini RAG Demo (what youâ€™ll get in the repo)

* `ingest.py` â€” loads sample docs, splits, embeds (MiniLM), indexes to **Chroma**.
* `app.py` â€” FastAPI with `/qa` (retrieval + simple generator); optional OpenAI plugâ€‘in.
* `rag.py` â€” helpers for embed/query; hybrid ready stub.
* `data/` â€” a few example knowledge chunks.
